{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Math\n",
    "import logging\n",
    "\n",
    "for h in logging.getLogger().handlers:\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(message)s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking an algorithm used to solve a MAX-k-SAT problem\n",
    "\n",
    "In this notebook we showcase functionality of qubrabench by implementing and benchmarking a hillclimbing algorithm to solve MAX-k-SAT, as described in https://arxiv.org/abs/2203.04975.\n",
    "<!-- The paper describes two distinct variants of implementing the hillclimber: a simple hillclimber - which uses (quantum) search, and a steep one - which uses (quantum) max finding. -->\n",
    "\n",
    "## Problem: MAX-k-SAT\n",
    "\n",
    "Max-k-SAT is a combinatorial optimization problem that given a list of clauses $(C_{i})^{p}_{i=1}$, each a disjunction of at most $k$ literals, and a set of weights $(w_{i})^{p}_{i=1}$, asks us to maximize the weight of the satisfied clauses,\n",
    "$$\\varphi(x) := \\sum ^{p} _{i=1} w_{i}C_{i}(x),$$\n",
    "over all assignments $x \\in \\{0, 1\\}^{q}$ of the variables. This problem is known to be NP-hard for $k â‰¥ 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Clause:\n",
    "    terms: Sequence[int]\n",
    "    weight: int\n",
    "\n",
    "    def is_satisfied(self, assignment: Sequence[bool]) -> bool:\n",
    "        return all(assignment[abs(x) - 1] == (x > 0) for x in self.terms)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Instance:\n",
    "    n_var: int\n",
    "    clauses: Sequence[Clause]\n",
    "\n",
    "    def evaluate_weight_of_assignment(self, assignment: Sequence[bool]) -> int:\n",
    "        \"\"\"compute \\varphi(assignment)\"\"\"\n",
    "        return sum(\n",
    "            clause.weight for clause in self.clauses if clause.is_satisfied(assignment)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Instance\n",
    "\n",
    "Consider the following instance for MAX-3-SAT, with 4 variables $x = (x_{1}, x_{2}, x_{3}, x_{4})$: \n",
    "\n",
    "$$(\\neg x_{1} \\lor \\neg x_{2} \\lor x_{3})_{3} \\land (x_{1} \\lor x_{2} \\lor \\neg x_{4})_{5} \\land (\\neg x_{1} \\lor \\neg x_{2} \\lor x_{4})_{1}$$\n",
    "\n",
    "In the course of the following sections, we will build an algorithm for MAX-$k$-SAT and use this instance to demonstrate how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_inst = Instance(\n",
    "    n_var=4,\n",
    "    clauses=[\n",
    "        Clause(terms=[-1, -2, 3], weight=3),\n",
    "        Clause(terms=[1, 2, -4], weight=5),\n",
    "        Clause(terms=[-1, -2, 4], weight=1),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this representation, we store the index $i$ for each variable $x_i$ in the clause. A negated literal $\\neg x_i$ is stored as $-i$.\n",
    "\n",
    "We can evaluate $\\varphi(x)$ for a given $x$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_inst.evaluate_weight_of_assignment((1, 1, 0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: Hillclimber\n",
    "\n",
    "<!-- A known algorithm to solve this problem is the hillclimber algorithm, which we will be implementing in this notebook. -->\n",
    "We start with a random assignment $x \\in \\{0, 1\\}^{n}$.\n",
    "We then repeatedly look for improvements (higher values) of $\\varphi$ in the _neighbourhood_ $N(x)$ of the current assignment $x$, and pick one if it exists (i.e. set $x$ to the chosen neighbour, and repeat).\n",
    "Here, $N(x)$ is the set of all assignments that differ from $x$ in exactly one bit.\n",
    "\n",
    "We can have different variants of the hillclimber based on how we pick a _better_ neighbour.\n",
    "In this notebook, we discuss the **steep** hillclimber: pick the neighbour $y$ with the highest value of $\\varphi(y)$.\n",
    "$$x' = \\arg\\max_{y \\in N(x)}(\\varphi(y))$$\n",
    "If $\\varphi(x') > \\varphi(x)$, we set $x \\leftarrow x'$ and repeat, otherwise we stop.\n",
    "\n",
    "<!-- ## Solving our problem instance\n",
    "\n",
    "As mentioned above, we can use a hillclimber algorithm to solve the MAX-k-SAT problem. For the purpose of simplicity within this notebook, we will be focussing on the steep hillclimber. Side note: This repository also contains a [hillclimber algorithm](https://github.com/qubrabench/qubrabench/blob/notebooks/examples/sat/bench_hillclimber.py), which uses a more advanced approach and has much better performance than our approach. However, as we are focussing on the functionality of qubrabench in this notebook, this more complex implementation will not be covered in more detail.\n",
    " -->\n",
    " <!-- \n",
    "We will assume, that $d=1$, which enables us to simplify some of our functions. This means, when we look at the neighbourhood of a given variable assignment $y$, we will always be looking at assignments $z\\in N_{1}(y)$, that differ from our current assignment in exactly one place. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our starting assignment, we implement a function which randomly generates a sequence of zeros and ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_assignment(n_var: int, *, rng=np.random.default_rng(42)):\n",
    "    return tuple(rng.choice((0, 1), size=n_var))\n",
    "\n",
    "\n",
    "random_assignment = generate_random_assignment(demo_inst.n_var)\n",
    "print(f\"Random assignment: {random_assignment}, which means:\")\n",
    "\n",
    "Math(\n",
    "    \", \".join(\n",
    "        f\"x_{i + 1} = {random_assignment[i]}\" for i, x_i in enumerate(random_assignment)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to compute the neighbourhood set $N(x)$ for a given assignment $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours(x):\n",
    "    neighbours = []\n",
    "    for i, x_i in enumerate(x):\n",
    "        y = list(x)\n",
    "        y[i] = 1 - x_i\n",
    "        neighbours.append(tuple(y))\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "print(get_neighbours((0, 0, 0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pick the _best_ neighbour for a given assignment $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_neighbour(x: Sequence[bool], inst: Instance):\n",
    "    return max(\n",
    "        get_neighbours(x),\n",
    "        key=inst.evaluate_weight_of_assignment,\n",
    "    )\n",
    "\n",
    "\n",
    "best_neighbour = find_best_neighbour((0, 0, 0, 0), demo_inst)\n",
    "neighbour_weight = demo_inst.evaluate_weight_of_assignment(best_neighbour)\n",
    "\n",
    "print(f\"Best neighbour: {best_neighbour}, weight = {neighbour_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above subroutines defined, we can now implement the hillclimber algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climber_sat(\n",
    "    inst: Instance,\n",
    "    *,\n",
    "    rng=np.random.default_rng(42),\n",
    "):\n",
    "    # starting assignment\n",
    "    x = generate_random_assignment(inst.n_var, rng=rng)\n",
    "    weight = inst.evaluate_weight_of_assignment(x)\n",
    "    logging.info(f\"Starting with assignment: {x}, with weight {weight}\")\n",
    "\n",
    "    while True:\n",
    "        x_new = find_best_neighbour(x, inst)\n",
    "        neighbour_weight = inst.evaluate_weight_of_assignment(x_new)\n",
    "        if neighbour_weight > weight:\n",
    "            x, weight = x_new, neighbour_weight\n",
    "            logging.info(f\"Found better neighbour: {x}, with weight {weight}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    logging.info(f\"Completed. Optimal assignment: {x}. Weight: {weight}\")\n",
    "    return x\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "hill_climber_sat(demo_inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotting a pattern: Max\n",
    "\n",
    "If we take another look at this approach, we will notice, that we calculate the maximum value of a set of numbers within the find_better_assignment function. In this case the maximum weight amongst the weights of all neighbours using the Python max function. \n",
    "\n",
    "As it happens there is also a very well known max function, which runs on quantum computers, as described in the paper mentioned in the introduction. With this in mind, the question naturally occurs, what would happen if we ran the computation of max on a quantum machine using this quantum max function and if this would have any performance improvements.\n",
    "\n",
    "This library offers a function, which enables a user to simulate doing exactly this and to keep track of runtime stats and approximations for quantum call counts, if the quantum max function was used and run on a quantum machine.\n",
    "\n",
    "## Max Function\n",
    "\n",
    "The library includes a set of common functions you would need in various circumstances, for example the search and the max function. These return the result you would expect from other implementations of these functions, and at the same time add in quantum benchmarking functionality. This simultaneously makes it easier to understand the use-cases of the library and makes it relatively easy to adapt existing code to use quantum benchmarking.\n",
    "\n",
    "When we talk about quantum benchmarking here, we mean approximating the amount of calls needed to execute the function with identical parameters on a quantum computer. This library achieves this by counting the calls run on the classical (in this case most likely your) machine and basing performance assumptions on these measurements. All provided functions in qubrabench currently require a stats object, in which these collected statistics and approximations are stored.\n",
    "\n",
    "Let's take a look at the docstrings of the function we will need to solve our problem - max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubrabench.algorithms.max import max as qmax\n",
    "\n",
    "%pdoc qmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will notice, the arguments for this max function are very similar to the Python max function. Indeed, one could run this function instead of max with the same parameters and would still get the wanted functionality. However, it is also possible to supply this function with two additional arguments - error and stats - which enables us to keep track of the calculated statistics.\n",
    "\n",
    "To demonstrate this, we will rewrite the find_better_assignment function from above to use the qubrabench max function. This is simply done by importing this function with the name max. We also create a stats object as provided by qubrabench and pass this to the max function together with a value for the error argument. Now we can run the same hill_climber_sat function we defined above and take a look at the stats object after it ran using the redefined find_better_assignment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubrabench.algorithms.max import max\n",
    "from qubrabench.stats import QueryStats\n",
    "\n",
    "\n",
    "stats = QueryStats()\n",
    "\n",
    "\n",
    "def find_best_neighbour(x, inst: Instance):\n",
    "    return max(\n",
    "        get_neighbours(x),\n",
    "        key=inst.evaluate_weight_of_assignment,\n",
    "        error=10**-5,  # new parameter!\n",
    "        stats=stats,  # new parameter!\n",
    "    )\n",
    "\n",
    "\n",
    "hill_climber_sat(demo_inst)\n",
    "\n",
    "print()\n",
    "print(f\"Classical Actual Queries: {stats.classical_actual_queries}\")\n",
    "print(f\"Expected Quantum Queries: {stats.quantum_expected_quantum_queries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example, the effort to use the qubrabench max function was very minimal and hardly invasive in our code. At the same time, it enabled us to gain valuable insight into our query statistics regarding the use of a quantum algorithm. As we can see, using the quantum max algorithm would worsen our runtime for this small problem instance. However, there are cases in which there are expected benefits for the runtime. These only occur in very large problem instances, which would take very long to run with our hillclimber implementation.\n",
    "\n",
    "In the following section we will be running the hillclimber provided in this library, because it runs much faster for larger problem instances. This will nonetheless be sufficient to demonstrate how these benchmarking results may look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Benchmarking\n",
    "\n",
    "The imported `run` function executes the hillclimber on a random instance and returns the statistics in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "def run(\n",
    "    k: int,\n",
    "    n: int,\n",
    "    r: int = 3,\n",
    "    *,\n",
    "    n_runs: int,\n",
    "    rng: np.random.Generator,\n",
    "    error: Optional[float] = None,\n",
    "    steep: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    assert n_runs >= 1\n",
    "    assert 3 <= r <= 6\n",
    "\n",
    "    clause_count = n * r\n",
    "\n",
    "    history = []\n",
    "    for run_ix in range(n_runs):\n",
    "        logging.info(f\"Instance: k={k}, r={r}, n={n}, steep={steep}, #{run_ix}\")\n",
    "\n",
    "        global stats\n",
    "        stats = QueryStats()  # reset\n",
    "\n",
    "        # Generate random problem instance\n",
    "        inst = Instance(\n",
    "            n_var=n,\n",
    "            clauses=[\n",
    "                Clause(\n",
    "                    terms=rng.choice([-1, 1])\n",
    "                    * rng.integers(1, n, endpoint=True, size=k),\n",
    "                    weight=rng.integers(1, 21),\n",
    "                )\n",
    "                for _ in range(clause_count)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        hill_climber_sat(inst)\n",
    "\n",
    "        # save record to history\n",
    "        rec = asdict(stats)\n",
    "        rec[\"n\"] = n\n",
    "        rec[\"r\"] = r\n",
    "        rec[\"k\"] = k\n",
    "        history.append(rec)\n",
    "\n",
    "    # return pandas dataframe\n",
    "    df = pd.DataFrame(\n",
    "        [list(row.values()) for row in history], columns=list(history[0].keys())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARN)\n",
    "run(3, 10, 3, n_runs=5, rng=np.random.default_rng(seed=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, we can run the hillclimber multiple times and then compute the average of the results.\n",
    "\n",
    "Let's run the steep hill climber for $n = 100$, $n = 300$ and $n=500$. We will run the hill climber five times for each $n$ and consistently use $k = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_100 = run(\n",
    "    k=3,\n",
    "    r=3,\n",
    "    n=100,\n",
    "    n_runs=5,\n",
    "    rng=np.random.default_rng(seed=100),\n",
    "    error=10**-5,\n",
    "    steep=True,\n",
    ")\n",
    "data_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_300 = run(\n",
    "    k=3,\n",
    "    r=3,\n",
    "    n=300,\n",
    "    n_runs=5,\n",
    "    rng=np.random.default_rng(seed=100),\n",
    "    error=10**-5,\n",
    "    steep=True,\n",
    ")\n",
    "data_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_500 = run(\n",
    "    k=3,\n",
    "    r=3,\n",
    "    n=500,\n",
    "    n_runs=5,\n",
    "    rng=np.random.default_rng(seed=100),\n",
    "    error=10**-5,\n",
    "    steep=True,\n",
    ")\n",
    "data_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the resulting tables, for problem instances with $n \\le 500$ we can still achieve no performance improvement in using the quantum max algorithm. However, this would change at some point with $n>10^5$. The exact point for the steep hillclimber has not been calculated in the paper and it would take quite a while to do so, however for the simple hillclimber we see performance benefits around $n=10^4$. \n",
    "\n",
    "The values mentioned above are gained from the plots provided in the paper (page 28). Plots are a good way to visualize this kind of data, which is why qubrabench provides some functionality, which is supposed to make plotting these results easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "After we have benchmarked the solving of a couple of our problem instances, we can take a look at the plotting functionality qubrabench provides. We use the `PlottingStrategy` wrapper to define our plot parameters and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qubrabench.utils.plotting_strategy import PlottingStrategy\n",
    "\n",
    "\n",
    "class Plotter(PlottingStrategy):\n",
    "    def __init__(self):\n",
    "        # TODO: explain\n",
    "        self.colors[\"\"] = \"blue\"\n",
    "\n",
    "    def get_plot_group_column_names(self):\n",
    "        return [\"k\", \"r\"]\n",
    "\n",
    "    def get_data_group_column_names(self):\n",
    "        \"\"\"\n",
    "        Generate a data line for each unique value in the specified columns.\n",
    "        Useful if you the data was generated with different tags based on implementation source, parameter choice etc., that one wants to compare against in a single plot.\n",
    "\n",
    "        Example: [\"impl\"] - a line will be generated for each unique `impl` label.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def compute_aggregates(self, data, *, quantum_factor):\n",
    "        # compute combined query costs of quantum search\n",
    "        c = data[\"quantum_expected_classical_queries\"]\n",
    "        q = data[\"quantum_expected_quantum_queries\"]\n",
    "        data[\"quantum_cost\"] = c + quantum_factor * q\n",
    "        return data\n",
    "\n",
    "    def x_axis_column(self):\n",
    "        return \"n\"\n",
    "\n",
    "    def x_axis_label(self):\n",
    "        return \"$n$\"\n",
    "\n",
    "    def y_axis_label(self):\n",
    "        return \"Queries\"\n",
    "\n",
    "    def get_column_names_to_plot(self):\n",
    "        return {\n",
    "            \"classical_actual_queries\": (\"Classical Queries\", \"o\"),\n",
    "            \"quantum_cost\": (\"Quantum Queries\", \"x\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put all the benchmark stats in a single table and run the plotter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdoc Plotter.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_100, data_300, data_500])\n",
    "Plotter().plot(data, quantum_factor=2, y_lower_lim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also run the \"simple\" hillclimber for the above instance sizes, and compare the two benchmarks. We use the library function provided by qubrabench here, as we have not implemented the simple hillclimber here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hillclimber import run as run_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_simple = [\n",
    "    run_lib(\n",
    "        k=3,\n",
    "        r=3,\n",
    "        n=n,\n",
    "        n_runs=5,\n",
    "        rng=np.random.default_rng(seed=100),\n",
    "        error=10**-5,\n",
    "        steep=False,\n",
    "    )\n",
    "    for n in [100, 300, 500]\n",
    "]\n",
    "data_simple = pd.concat(data_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an extra column to distinguish the source (i.e. type of hillclimb)\n",
    "full_data = []\n",
    "for d, is_steep in [(data, True), (data_simple, False)]:\n",
    "    d = d.copy()\n",
    "    d.insert(0, \"steep\", is_steep)\n",
    "    full_data.append(d)\n",
    "full_data = pd.concat(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the above plotter a bit as we now want to group the data by column \"steep\" (in the same plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPlotter(Plotter):\n",
    "    def __init__(self):\n",
    "        self.colors[\"steep = False\"] = \"red\"\n",
    "        self.colors[\"steep = True\"] = \"blue\"\n",
    "\n",
    "    def get_data_group_column_names(self):\n",
    "        return [\"steep\"]\n",
    "\n",
    "\n",
    "FullPlotter().plot(full_data, quantum_factor=2, y_lower_lim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, we need less queries for the simple hillclimber for our problem instances. The simple hillclimber also shows more apparent possibilities for performance improvements through quantum algorithms, specifically Grover search in that case. This can be seen even more clearly in the plots provided in this paper on page 28: https://arxiv.org/abs/2203.04975"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
